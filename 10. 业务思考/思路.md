业绩成果：
1. 用户理解算法基础设施改进， 年龄性别指标提升，多个新标签上线
2. wifi信息挖掘应用，打通TV和移动端设备，识别用户场景，挖掘用户关系

讲演思路：

核心目标：
服务于用户理解，主要负责的方向挖掘各种设备上的标签，包括设备的基础属性标签，比如年龄性别，人生阶段，消费能力，以及兴趣类标签，比如商业兴趣。
以及关系类标签，比如tv端与移动端的打通关系。 为各个业务垂线的精细化运营及推荐广告金融风控等业务服务。

在过去的一年里，随着技术的积累和迭代。我们对算法基础设施做了多方面的优化，对我们的产出有全面的促进。

首先我们实现了标准化的产出流程，并在我们多个步骤中做了针对性的改进。

这个是我们改造后的流程图，深色部分是优化后的点。

![改造后的流程](D:\工作相关\其他积累\答辩\2020-08\改造后的流程.png)

首先特征方面，我们构建了通用的特征库（包含之前存在的离散系数特征和新增的统计特征），并且实现了特征的分域存储和按域进行选配合并，不同的标签模型可以有不同的组合配置。

这个特征库构建的骨架是分层配置，要点主要有三个，高覆盖，达不到覆盖要求的特征一般无法起到作用。独立性，不少特征具有很强的相关性，添加这种特征无法带来更多的信息量。第三个就是要融合第三方的知识，现成的特征基本上已经没有什么可以挖掘的了，通过融合第三方知识库，把离散的手机型号特征映射成连续的手机价格，把离散GPS点聚合映射成房价，这些特征能从更多角度刻画用户，带来了更独立更多的信息量。

主要是由于我们标签种类很多，有些特征可能完全不相关，有些特征可能会造成data leakage。我们的稀疏特征中有些维度很高，而有些标签的样本量很有限，所以仅仅能使用相对低维的特征。

除此以外，为了降低特征维度，我们也集成了一些，高效的通用组件，包括频次过滤，按与样本的相关性筛选，模型筛选等三类，这个步骤也是我们特征产出标准化的重要一环。因为目前我们特征库的维度在千万，多个标签模型样本量最大的也才几百万。

其次样本方面，画像中的样本主要存在几类问题，多种来源，样本少，噪声多的特点，他不像广告和推荐业务，样本就是事实点击来判断的。怎么在保证样本数量的前提下，尽可能提高样本质量是我们面临的主要问题，我们这里提炼了三种方法，去针对这个情况。

最后是我们把常用模型的训练和预测也标准化了，并对整个流程进行了监控。从特征维度变化，覆盖变化，样本分布变化等全部中间流程的关键数据都做了可视化监控和报警，这个是基于BI报表做的。这些措施对于我们排查指标波动，保证标签稳定非常重要。

1. *<!--面临的问题-->*
	1. *<!--缺乏通用性好的特征工程基础设施-->*
		1. *<!--通用高覆盖可配置的特征库-->*
		2. *<!--特征筛选机制 （tgi, 模型筛选，覆盖筛选）-->*
		3. *<!--特征覆盖监控体系-->*
    2. *<!--样本量受限，样本质量不可靠 （多来源交叉验证, 威尔逊区间分数，置信学习筛选样本)-->*
    3. *<!--模型输入输出不统一（tfrecord，parquet）-->*
    4. *<!--产出的标签的稳定性可控性-->*

2. *<!--解决思路-->*
	*<!--架构图-->*

工作流程图
特征阶段处理阶段 |  样本阶段 | 模型训练阶段 | 预测产出阶段

数据层次，和产出体系
原始数据来源 | 知识库的应用 | 

产出结果

模型研究：

为什么选择widedeep？

因为原来的模型是FM，和FM+XGB的组合。最开始我们也尝试过的一般的DNN，确实比简单的FM效果要好一些，指标也更加鲁棒。也就证明了深度模型确实能带来提高。

相对于CV领域的模型，推荐领域的模型更加合适一点我们的场景。

我们最早尝试的是widedeep， 一方面我积累了一些离散特征和统计特征都比较方便的利用起来，另外一方面我们也积累一些交叉的pattern，比如说城市线级和观影时间的交叉是比较能够反映用户的年龄的，以及某些app能够反映用户的婚育状况，在交叉上城市线级也很能反映用户的年龄。

除此以外，推荐场景其他的流行模型，DCN和DeepFM思路都是自动做特征交叉。我们也尝试了deepfm，并没有做出比widedeep更好的效果。 另外还有像阿里的DIN，这个的话不符合我们的场景，我们这个场景里面没有attention和用户行为序列的概念。所以目前我们线上运行的还是Widedeep。后续的话，我们可能会尝试多目标，比如把年龄性别放进来一起预测，可能可以利用一些男女婚育年龄差别的这种特点。

另外还有一个，跟推荐场景相比，我们的样本量要小多了。一般推荐场景，我了解我们公司一般样本都是过亿级别的，甚至widedeep google的那个原作者论文里面说他们用5000亿样本。目前我们样本量最大也就几百万，所以我们的TA模型，实际上在wide层还加了一层独立的模型特征筛选， 因为原始的特征维度相对于我们的维度有些太大了，加了特征筛选的要比不加效果要好。

附：因为进行了特征筛选了以后，有一部分用户就可能仅仅从wide侧，按照memoration的方式无法记忆，那么就需要靠deep侧的generation来推测。统计特征的有值率要大大高于离散特征部分。

