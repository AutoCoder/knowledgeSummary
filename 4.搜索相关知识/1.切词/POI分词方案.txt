**语料问题**﻿

​	由于之前的thulac分词库是基于人民日报的语料训练出来的，所以不适应POI的切词的场景。

​	比如 “日月光” -> “日|月光”

​	所以后面采用了POI的原始数据进行训练。

**离线POI搜索的特点**

1. 短文本，无长程相关性
2. 相邻词之间往往也可以独立存在
3. 速度要求高
4. 无需考虑新词（未录入词）
5. 分词粒度小
6. 切词模型体积不能大

基于这几条和实现复杂度的综合考虑，所以并没有采用HMM，MEMM, CRF这几种方法。

[HMM&MEMM&CRF参考文章]( https://www.52ml.net/1709.html)

**方案演进**

第一次尝试采用的方案是，基于词库的逆向最大匹配算法。

基于切词速度和数据量的的考虑，采用双数组Trie树(DAT)进行存贮。这样可以不需要进行二分查找（时间复杂度为M * LogN，N为词库的大小, M为切词句子的长度），而DAT的时间复杂度仅仅为M。

假如词库大小为100w，那么DAT的效率为一般二叉树的 log1000000 = 20倍。

基于这种方案的切词结果，虽然用的是POI的源数据进行训练的，解决了 “日月光” -> “日|月光”的问题。但是也存在一些其他的问题，比如：

> ​	北京黑龙潭自然风景区=>{北京|黑龙|潭自然|风景区}
>
> ​	康陵陵=>{康|陵陵} （故意输错, 会导致“康”未实现匹配，整个切词熵为0）

针对Case 1:

	可以明显看出，“潭自然” 这个词应该概率较低，且“黑龙”这个词概率也应该比较低。 而 “黑龙潭” 和 “自然” 概率应该都相应高很多。
	正确切词和错误切词的联合概率是相差很大的，这里如果引入最大熵的概念应该能很好的解决这个问题。
	
	############################################ 
		熵的公式 =》  H = -p(x)log(p(x)) 
	############################################ 
	
	假设词库大小为N，某一个词的频率为x, 那么它的熵为 x/N * log(N/x) = x/N * (logN - logx), 去掉公倍数 为 x(logN-logx).


​	

**最终方案&优势**

因为POI都是短文本，切词方案不会很多，可以穷举出所有的切词方案，然后算出熵最大的切词方案。相对于BMES字标注 的 HMM算法，这个算法的好处就是考虑了全局，且只基于DAT格式的词库数据就能实现，不需要重新训练HMM数据模型。且DAT数据未来可以用于sug检索。

但是经过试验，对两千五百万的poi的原始数据进行切词发现，还是有不少非常长的POI地址或者名称，长度可能达到50个字。穷举在面对这种case的情况，非常慢。切词需要几十天才能完成。

于是采用了动态规划(viterbi 维特比算法)的方式去计算熵最大的切词方案，计算速度大大提升，但是面对特别长的POI地址，仍然十分慢，对于这种情况，切换到逆向最大匹配算法进行切词。这种算法的时间复杂度是线性的，
只和输入字符串的长度有关。通常过于长的输入只会在数据编译阶段出现，车机用户往往输入的是短词，往往遇不到逆向最大匹配算法的问题。

**改进切词粒度问题**

​	由于高德的原始数据中，虽然所有的拼音都是人工输入的，但是常常伴随一个问题，就是不少很长的词，这对于检索的召回是非常不利的。所有在数据模型构建之前，对于所有的长词，利用第三方
	jieba分词进行了二次切割，再利用切割后的语料构建切词模型。解决了长词难以召回的问题。



**解释介绍**

开发的这个方法采用的是基于词库的最大联合概率的方法。

我接手前使用的是Thulac的一个开源切词库，这个切词库采用的是MEMM模型，提供了一份人民日报语料训练的模型，可想而知切出的词有很多问题，比如“日月光”切成“日”，“月光”等等， 我第一步先是把原始poi的语料准备出来，用Thulac 的训练程序来训练，每次都是异常的coredump。因为我们有2500w条poi，且有不少字段，所以语料比较大，这个开源的程序毕竟不是商业的，处理不了。

我先尝试改进这个问题，于是我就先调研了一下比较常见的HMM, MEMM, CRF，了解了一下它的原理。

这三种都是对字进行标注的做法。

HMM是一种生成式的模型，统计的是P(X,Y)的联合概率，一般是得到四个矩阵，start_probability, transition_probability, emit_probability, 然后假定当前状态只依赖于前一个状态，当前观测只依赖于当前状态，然后通过维特比求最大联合概率的路径。

MEMM是一种判别式的模型，他对P(Y|X)用最大熵的方式进行建模，然后得到条件概率，再根据维特比求解最大概率的路径，CRF和这个类似，只是CRF是无向图模型，在计算条件概率的时候，是所有最大团势函数的联合概率，解决了MEMM的标注偏置的问题。MEMM和CRF都可以定义一些 unigram和bigram的一些二值特征函数作为最大熵模型的等式约束，不再受到HMM的独立性假设的限制。

但是我们的poi数据和其他的语料比如人民日报的语料区别很大，因为poi的语料往往不是一句话，而且很多词并不是真正的词，比如明申大厦这种，出现的概率很低，也没有什么长程相关性。 这种二值特征函数我感觉也用不上。其实一般的基于词库的最大逆向匹配都可以表现的不错，还是还是不少case会有问题，然后我维特比方法求词频率的联合概率最大的分词方法，把所有badcase的切词问题都解决了。

然后我借鉴了thulac中的双数组tree树的结果，把所有的词频通过DAT来记录，可以让非常快的查找词频进行动态规划，对于相同前缀的词，一次查询就可以他们的词频都取出来，而不用多次进行二分查找。