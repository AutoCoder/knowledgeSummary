#### 理论问题：

1. 偏差，方差解释
    而bias和variance分别从两个方面来描述了我们学习到的模型与真实模型之间的差距。
    Bias是 “用所有可能的训练数据集训练出的所有模型的输出的平均值” 与 “真实模型”的输出值之间的差异；
    Variance则是“不同的训练数据集训练出的模型”的输出值之间的差异。

2. 为什么除了训练集和测试集，还有验证集，什么时候需要验证集？
    因为如果用整个test data做调参的话，调出来的数据肯定最适合test data的，而这个数据是否对于其他数据合适，
    我们没有办法知道，所以需要从原始数据中划出一部分当验证集。另外还有一种情况，测试集来自线上真实环境，验证集来自训练数据中随机抽取的一部分
    
3. 过拟合，欠拟合

    前者：训练集指标明显好于测试集，说明模型过于拟合了训练集的特点，不能很好的泛化到测试集

    后者：训练集指标明显过低，不能拟合训练集

4. 集成学习，bagging，boosting, stacking

    bagging的代表，随机森林。主要思路是， 降低单棵树的偏差，然后通过一片树林的投票来客服单棵数带来的方差。要求每个树好而不同，所以对样本进行采样，对特征也进行一定的采样，最终达到好而不同的目的。树越多，模型的方差就越收敛。

    boosting的代表，gbdt，xgboost，lightgbm。boosting的主要思路是，每棵树持续拟合上一棵树输出的残差，不断降低残差，每棵树不能走的太偏，对单棵的偏差要求没有bagging的高，所以树的深度一般比bagging的树深度小（boosting 5-8, bagging 可以到20）

    stacking是一种模式，对于多种模型的结果做融合，把多个模型的输出概率作为输入到融合模型种去，比如FM的结果输出到xgboost中去

5. 二分类评估指标
    pr : TP / TP + FP
    re : TP / TP + FN
    acc  : TP + TN /  TP + TN + FP + FN
    roc:  坐标系 FPR（负类召回率） (x轴） TPR（正类召回率）（y轴）  当threshold不断变小，预测为正的不断变多时，画出的曲线 
    auc :  The area of ROC， 古典概型就是 任选两个预测，p(正例 >负例）
    gauc ： 在同一个group(用户）中， 任选两个预测 p(正例 >负例）， 然后按照 占比加权得到的指标

6. AUC如何计算，gAUC如何计算

```
label_arr = [1,1,1,1,0,0,0,0,0]
prob_arr = [0.8,0.4,0.5,0.2,0.35,0.45,0.25,0.36,0.78]
group_arr = [1,1,2,2,1,1,2,2,2]

def auc(label_arr, prob_arr):
    tup_arr = zip(label_arr, prob_arr)
    pos_num = np.count_nonzero(label_arr)
    neg_num = len(label_arr) - pos_num
    sort_arr = sorted(tup_arr, key=lambda tup: tup[1])
    sort_correct_num = 0
    neg_num = 0

    for i in range(len(sort_arr)):
        if (sort_arr[i][0] == 1):
            sort_correct_num += neg_num
        else:
            neg_num +=1
            
    return sort_correct_num / (pos_num * neg_num)
    
def gauc(label_arr, prob_arr, group_arr):
    group_data_dict = {}
    for idx in range(len(group_arr)):
        if group_arr[idx] not in group_data_dict.keys():
            group_data_dict[group_arr[idx]] = {"label_arr" : [], "prob_arr" :[]}
        
        group_data_dict[group_arr[idx]]["label_arr"].append(label_arr[idx])
        group_data_dict[group_arr[idx]]["prob_arr"].append(prob_arr[idx])
           
    print(group_data_dict)
    
    numerator = 0.0
    denumerator = 0.0
    for k, v in group_data_dict.items():
        denumerator += len(v['label_arr'])
        cur_auc = auc(v["label_arr"], v["prob_arr"])
        print("cur_auc:" + str(cur_auc))
        numerator += cur_auc * denumerator
        
    print("numerator:" + str(numerator))
    print("denumerator:" + str(denumerator))
    return numerator / denumerator

gauc(label_arr, prob_arr, group_arr)
```

7. 交叉熵损失函数？ 为啥要用交叉熵作为Loss进行优化

   Answer1： 
   
   交叉熵的一般形式： $\displaystyle L= -\sum_{i=0}^{m}y_ilog(q_i) $ 其中 $p$ 为样本后验概率， $q$ 为 模型预测概率, $m$ 为样本数
   
    $y_i$ 代表了样本分布
   
    $q$ 代表了预测分布
   
   当交叉熵的值越小时，两个概率分布也就越接近，实际与期望的差距也就越小。 越说明模型学习的越好
   
   对于2分类：$L = -[ylog(p) + (1-y)log(1-p)]$
   
   对于k分类： $\displaystyle L= -\sum_{i=0}^{k}y_ilog(q_i) $
   
   首先 $L$是非负的，因为$log(q_i) < 0,  y \in (0,1) $，
   
   其次 $q_i$越接近$y_i$ 则交叉熵越小
   
   Answer2:
   
   首先交叉熵是等价与极大似然估计。
   
   极大似然估计到交叉熵的推导=>

​        $$\displaystyle L(\theta ) = \prod_{i=0}^{m}P(y^{(i)}|x^{(i)};\theta ) \qquad  \Rightarrow \qquad    \prod_{i=0}^{m}((1 - h_{\theta }(x^{(i)}))^{1-y^{(i)}} * h_{\theta }(x^{(i)})^{y^{(i)}})$$

​        连乘取Log变连加，对于多个样本方便优化

​         $$\displaystyle l(\theta) = logL(\theta ) = \sum_{i=0}^{m}(y^{(i)}log(h_{\theta }(x^{(i)})) + (1-y^{(i)})log(1 - h_{\theta }(x^{(i)})))$$

​        其次, 交叉熵的求导比较简单：

​        $\displaystyle\sigma(Z) = \frac{1}{1+e^{-Z}} = \frac{e^{Z}}{1+e^{Z}}$   

​        $\displaystyle \frac{\partial L}{\partial W_i} = \frac{\partial L}{\partial P} \frac{\partial P}{\partial Z} \frac{\partial Z}{\partial W}$

​       $$\displaystyle \frac{\partial L}{\partial P} = - y\frac{1}{P} - (1-y)\frac{1}{1-p} $$

​      $$\displaystyle \frac{\partial P}{\partial Z} = \frac{e^{Z}}{1+e^{Z}} \frac{1}{1+e^{Z}} $$

​      $$\displaystyle \frac{\partial Z}{\partial W_i} = x_i $$ 

​       合起来 $= |\sigma(Z) - y| * x_i $ 误差较大时有更大的梯度

​        如果用mse来算 $L = \frac{1}{2}(\sigma(Z) - y)^2 $， 首先回归0，1样本，与分类的本质并不等价

​         $\displaystyle \frac{\partial L}{\partial W_i} = (p-y) \sigma(Z) (1- \sigma(Z))x_i $  会导致当$\sigma(Z)$接近0,1时 接近为0， 三个小于0的小数相乘，导致梯度一致很小。

8. 深度学习LOSS不收敛
    学习率过大 （来回跳跃）
    
    特征和label并没有相关性
    
    反向传播
    
     $$\displaystyle \frac{\varphi Loss}{\varphi W_1}\;=\;\frac{\varphi Loss}{\varphi(a_3)} * \frac{\varphi a_3}{\varphi(Z_3)}\; * \frac{\varphi Z_3}{\varphi(a_2)}\; * \frac{\varphi a_2}{\varphi(Z_2)}\;* \frac{\varphi Z_2}{\varphi(a_1)}\;* \frac{\varphi a_1}{\varphi(Z_1)}\;* \frac{\varphi Z_1}{\varphi(W_1)}\;$$
    
    $$\displaystyle = \frac{\varphi Loss}{\varphi(a_3)} * s'\;(Z_3) * W_3 * s'\;(Z_2) * W_2 * s'\;(Z_1) * W_1 * X$$
    
    梯度消失  $$s'\;(Z_n)$$ 较小导致连乘导致梯度过小 （排查：打印某一层的输出, 解决：使用非饱和的激活函数）
    
    梯度爆炸  $W_n$ 过大，连乘导致梯度太大，但是一般$W$合理初始化，不太会出现这个问题（ 梯度截断 Gradient Clipping）
    
    不稳定梯度带来的风险不止在于数值表示。
    不稳定梯度也威胁到我们优化算法的稳定性。
    我们可能面临一些问题。 
    要么是 梯度爆炸（gradient exploding）问题：参数更新过大，破坏了模型的稳定收敛； 
    要么是 梯度消失（gradient vanishing）问题：参数更新过小，在每次更新时几乎不会移动，导致无法学习。
    
    更稳定（但在神经科学的角度看起来不太合理）的ReLU系列函数已经成为从业者的默认选择。
    因为梯度是一个常数，所以既不太可能爆炸也不太可能消失。
    
    另外参数初始化不宜太大，防止出现梯度爆炸
    
9. 深度学习中，loss中nan引起异常
   
    ```
    For example: Nan in summary histogram for: bpr_loss
    ```
    可能存在的原因：
    
    ​	梯度爆炸;   解决方法：调学习率、梯度剪裁、归一化
    
    ​    ![log函数](https://bkimg.cdn.bcebos.com/pic/b8014a90f603738d0e94646db11bb051f919eca7?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2UxMTY=,g_7,xp_5,yp_5/format,f_auto)
    
    ​	计算loss的时候有用到tf.log函数， 它的输入值特征接近于0， 导致输出为负无穷; 可能是初始化的问题，也可能是数据的问题
    
10. 深度学习过拟合问题
    模型过于复杂，样本不足
    特征共线性，使得特征冗余导致，每个特征上的系数学习的不稳定，导致过拟合，估计出现不准确
    解决方案： 数据增强，正则化，dropout, BN（可替代dropout）, 简化模型，stopping early

11. BN的做法，为什么BN具有正则化的效果？实现一个BN
     因为BN对神经元的线性变换输出Z，进行了batch内的标准化， 使得Z_hat = (Z - mean) / std 这大大减小了线性变换的输出值。
     本质上起到了L1和L2减小参数W的效果，过拟合的本质是某一个特征的过分表达，系数过大，大大影响了最后的输出。BN的存在，
     使得即使W很大，最后得到输出也不会被过拟合的特征过分影响。

     ```
     input = np.array([
         [1,2,3,4,5,6],
         [5,4,3,2,1,6],
         [2,3,2,2,3,6],
         [3,1,4,3,2,6],
         [4,5,1,5,4,6]
     ])
     
     #batch_size * 当前层的输出node数  => 按列做归一化
     parameter_dict = {
         "bn_mean" : [],
         "bn_var" : []
     }
     
     average_window = 100
     
     def batch_normilization_tf(input, mode, parameter_dict):
         input_tensor = tf.convert_to_tensor(input, tf.float32)
         epsilon = 0.000000001
         mean = None
         variance = None
         if mode == 'Train':
             mean, variance = tf.nn.moments(input_tensor, 0)
             parameter_dict["bn_mean"].append(mean)
             parameter_dict["bn_var"].append(variance)
         else if mode == 'Predict':
             mean = std::mean(parameter_dict["bn_mean"][-1*average_window:])
             variance = std::mean(parameter_dict["bn_var"][-1*average_window:])
             
         gamma = tf.Variable(tf.ones(tf.shape(mean), dtype=tf.float32), name='gamma')
         beta = tf.Variable(tf.zeros(tf.shape(mean), dtype=tf.float32), name='beta')
         Z_hat = gamma * (input_tensor - mean) / tf.sqrt(variance + epsilon) + beta   
         return Z_hat;
     
     batch_normilization_tf(input)
     ```

     

12. 实现一个dropout

```python
input = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 1.0, 2.0, 3.0, 4.0, 5.0, 1.0, 2.0, 3.0, 4.0, 5.0, 1.0, 2.0, 3.0, 4.0, 5.0])

def dropout_tf(input, keep_prob):
        input_tensor = tf.convert_to_tensor(input)
        keep_prob_tensor = tf.convert_to_tensor(keep_prob)
        mask_shape = tf.shape(input_tensor)
        random_array = tf.random_uniform(mask_shape, minval=0.0 ,maxval=1.0 ,dtype=tf.float32)
        mid_tensor = random_array + keep_prob_tensor
        mask = tf.floor(mid_tensor)
        out_tensor = input * mask
        return random_array, mid_tensor, mask, out_tensor
        
random_array, mid_tensor, mask, out_tensor = dropout_tf(input, 0.5)
with tf.Session() as sess:
    print(sess.run(random_array))
    print(sess.run(mid_tensor))
    print(sess.run(mask))
    print(sess.run(out_tensor))
    
import tensorflow as tf
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    if dropout == 1:
        return tf.zeros_like(X)
    if dropout == 0:
        return X
    mask = tf.random.uniform(
        shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout
    return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)
    
def dropout_np(input, keep_prob):
    random_array = np.random.uniform(0.0 ,1.0, np.shape(input))
    random_array2 = random_array + keep_prob
    print(random_array2)
    return np.floor(random_array2)

print(drop_out(input, 0.5))
```

13. BN在训练和预估的区别?

    对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。
    而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。

14. Dropout 在训练和预估时的区别?

    Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。
    而在测试时，应该用整个训练好的模型，因此不需要dropout。

15. 两者一起使用的问题
    不会1+1>2（可能会相互影响）, 业界经验一般使用BN后，则不太需要Dropout了

16. RNN 梯度消失有啥不同？
    RNN的梯度消失，主要指梯度由近距离梯度主导，远距离梯度被忽略，而不是一般意义上连乘导致进入饱和区导致梯度消失

17. BN和LN的区别？ 为啥transformer中使用LN而不是BN

    BN = batchsize norm
    LN = layer norm

    BN 是指在一个batch内，对同一个特征进行feature scaling.

    LN 是指在一个样本中，对每个特征norm，让均值为0，方差为1. 

    BN在推荐上比较适用，NLP任务中一般使用LR，首先因为NLP的输入（特征数）不定长，导致batch_size内特征的均值方差比较难算。第二个NLP任务中，每个特征其实都是语言，和推荐中那种同一个特征是观看了同一部album的含义不同。所以LN是比较适用的。

#### 实践问题：

1. hive 熟悉程度

```
表A
album_id   |  channel_id 
10001      |  2         

表B
user_id                              | album_id 
f91259c882ca7198f415254c1c49dc4c110e | 10001

表C
album_id   |      tags              | publish_date
10001      |  喜剧;穿越;爱情          |  2021-07-31
```

- 需要统计album_id的uv，得到每个频道最人们的TOP 10部剧
- 表A中的一些大剧会出现倾斜问题，怎么解决？
- 需要把表C的tag映射到用户上，怎么处理？
- 假设表C是推荐场景的黑名单，如何得到一个该场景的corpus

2. spark 的应用
   1. action算子 和 transformation算子 的区别？
      NarrowDependency 和 ShuffleDependency 的含义和区别？
   2. spark处理数据的默认分区数？
      处理文件和表时有区别
   3. sparkSession.sql(sql_str) 产出一个巨大的dataframe， 数据量非常大，但是出来的默认分区是200. 导致后续的并行度不够
      sparkSession.conf.set("spark.sql.shuffle.partitions", 64)
   4. OOM 如何分析和处理？

3. hive中如何获取中位数，基本算法原理是？ precentile ，写一个程序获取中位数？

4. 写一个多元线性回归

    ```
    boston = tf.contrib.learn.datasets.load_dataset('boston')
    x, y_ = boston.data, boston.target
    
    W = tf.Variable(tf.random_uniform([x.shape[1], 1],-1.0,1.0),name='W')
    b = tf.Variable(tf.zeros([1]),name='b')
    y = tf.matmul(tf.cast(x,tf.float32), W) + b
    
    loss = tf.reduce_mean(tf.square(y-y_),name='loss')
    
    // 采用梯度下降来优化参数
    
    optimizer = tf.train.GradientDescentOptimizer(0.5)
    train_op = optimizer.minimize(loss,name='train')
    
    with tf.Session() as sess:
        init = tf.global_variables_initializer()
        sess.run(init)
        
    for seg in range (100):
        sess.run(train_op)
    
    print ('W=', sess.run(W), 'b=', sess.run(b),'loss=', sess.run(loss)) 

5. 写一个快排，时间复杂度分析，如何避免有序数组导致最坏的复杂度

6. 写一个堆排，时间复杂度分析，一般用法

7. 线下训练指标提升 但是线上不提升

    1.  根本原因就是线上线下的数据并不一致，可能训练时使用了一些线上无法获取的特征（可能是特征穿越，可以做特征回放）
    2. 可能是评价指标有差异，比如线下一般看auc，线上A/B关注的是时长，这两者并不完全等价
    3. 也可能是线上数据变化太快，比如每天的热门视频都在快速变化，T+1的训练的模型，并不能在新的一天表现很好(FTRL在线学习)




#### 业务经验：

1. You May Also Like i2i的方式，讲讲i2i一般有哪些产出方式？

   1. 基于内容的方法：

      - tag召回
      - tag jarcard相似度	
      - up主召回
      - 热门
      - 榜单
      - LBS召回
   
2. 基于协同过滤的方法：

   - itemcf
   - 矩阵分解MF (ALS， SVD， 前者比较容易计算，spark中有现成的包)
   - swing
   - FM （二阶隐向量， FAISS召回）（据石塔西大佬说，效果不好，因为模型没见过世面，拿排序的样本做的）
   - item2vec (序列前后要有相关性）
        - youtubednn （label按照频率降序，通过齐夫分布进行采样（负采样中打压热门内容）, 负采样数越大越逼近完整softmax）（实践中，打压热门样本后反而效果变差了）（sample_softmax  比 nce_loss 更好，具有pairwise的效果）
   - dssm （in-batch 负采样， bpr 的loss计算方法）
        - dssm的在利用in-batch的负采样的情况下，要保证batch是不同的user和item，否则可能会出现bpr的内积矩阵大部分为1.0的情况(比如说一个batch内全部是赘婿的观影)，为了防止这种情况出现，有两种策略：
             -  tfrecord生成时，要做随机打乱（双塔的训练集，很可能按照item来做join）
             -  tensorflow训练的时候，dataset要做shuffle。（我之前为了避免预测时的shuffle，去掉了这部分）
   3. 融合两种方法的方案：
       - 先基于协同过滤的方案，用全站内容做itemcf, swing, als 产出多组i2i召回，然后利用半屏页的经典相关推荐（i2i）场景的样本，把tv_id 和 rec_tv_id的各种特征（召回来源和分数， 各种tag， 各种统计指标）组织起来做一个pairwise的排序模型。

3.  E&E(exploit && explore)方法

    1. $\epsilon-greedy$ 
    2. Thompson Sampling 汤普森采样
    3. UCB
    4. linUCB

4. U2I 与 I2I的异同

    - I2I的相关性更好，U2I 则看起来比较难以解释。但是U2I相对于I2I来说，没有trigger的选择问题，更端到端

5. 更前沿的一些召回模型
    - MIND 多兴趣向量（胶囊网络;  可以采用更简单的版本，产出多个user_embedding, 永远用 点积最大的输出到优化Loss中）
    - TDM (难落地)

6. 如何评价一路召回的质量？

    - recall_of_k 当召回池量级不变时具有一定的参考性，但是如果后验样本，本身比较局限，可能好的召回也会在recall_of_k上表现不好
    - 召回diff率
    - 精排占比
    - 召回胜出率
    - U2I 覆盖度 hitrate   
      - 设$R_i$是用户i最近的N个item集合， $T_i$是用户i 实际点击/下单过的item集合
      - $$\displaystyle hitrate = \sum \frac{R_i \cap T_i}{R_i}$$
    - AB Test

7. Attention 原理和种类？

    1. self-attention  (自己学一个参数)
    2. content-base attention （计算序列中的embedding和预测item的embedding的相似度作为weight）

8. 了解的排序模型？

    ​    了解模型，要了解这些模型上提出的业务背景。比如widedeep在google app应用市场上的推荐，其中wide层专门使用了展现app & 安装app的交叉离散特征，这个是几十万* 几十万的规模，非常稀疏强调记忆性（专门使用ftrl），据说用了千亿样本才训练出来。

    youtubednn模型，是针对的短视频推荐场景，召回上强调负采样，排序上使用了时长加权的weighted loss

    dcn也是google的ctr模型，主要核心优化点就是wide层替换位cross，强调特征交叉

    mmoe也是google的模型，也是针对视频推荐领域的，一般对ctr，vt，评分，转发等多个目标进行建模

    对应一个比较知名的esmm，则是广告领域的，主要是客服cvr训练和预估时分布不一致和样本稀疏的问题

    1. widedeep  

       1. 记忆性，install_app *  展现app， wide侧的实现（workaroud：特征筛选后用numerical feature column来做wide侧)

    2. [youtubednn](https://zhuanlan.zhihu.com/p/347300209)

       1. weighted_sigmoid_cross_entropy_with_logits 通过时长对样本进行加权 （效果不如mmoe)

    3. dcn([deep & cross](https://zhuanlan.zhihu.com/p/138358291))

       1. 稀疏类别特征embedding后并联连续特征，一边深度网络，一边是交叉残差网络（增强交叉能力）。除了embedding层，参数增加都不多；最后两个塔concat起来，生成logit做sigmoid

    4. mmoe（[Multi-gate Mixture-of-Experts](https://zhuanlan.zhihu.com/p/291406172)）

       1. 多任务的几个层次：

          1. **底层参数共享**：互相补充学习，任务相关性越高，模型的loss可以降低到更低。但如果相关性不强，Hard parameter sharing会损害效果
          2. **多个Experts替换底层参数共享(MOE)**,  通过一个gate结构来对多个expert的输出做加权
          3. **多个Experts多个Gate(MMOE)**, 一个gate对应一个任务， 这样的改进可以针对不同任务得到不同的 Experts 权重， 因此模型更容易捕捉到子任务间的相关性和差异性

       2. 实践做法：

          1. 两个目标，一个ctr和一个vt

          2. ctr用sigmoid做loss，vt用mse做loss （vt的预估值可以直接用原始的预估值，也可以乘以ctr_label得到一个vt期望）

          3. 最后两者加和 

             ```
             -- v1 线上版本
             ctr_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=ctr_output, labels=ctr_label)    
             vt_loss2 = tf.losses.mean_squared_error(vt_label, vt_y_hat)
             loss = ctr_all_loss + vt_loss2
             Comment: vt loss 中包含了没有点击的样本，是否存在SSB问题(sample selection bias)
             
             -- v2 理论更好的版本
             ctr_predict  = tf.nn.sigmoid(logits)
             vt_predict = ...
             predict = ctr_predict *  vt_predict
             label = ctr_label * vt_label
             loss = tf.losses.mean_squared_error(label, predict)
             
             ```

    5. ESMM（[Entire Space Multi-Task Model](https://zhuanlan.zhihu.com/p/370499860)）([蘑菇街](https://zhuanlan.zhihu.com/p/338515054))

       **样本选择偏差（SSB）:** 转化是在点击之后发生，传统CVR预估模型在click数据上训练，但是在推理时使用了整个样本空间。不符合机器学习中训练数据和测试数据独立同分布的假设。为什么不将未点击的样本作为负样本直接训练呢？因为不确定未点击的样本被点击之后是否被转化，将其置为0是不准确的； 

       **训练数据稀疏（DP）:** 点击样本在整个样本空间中只占了很小一部分，而转化样本更少，高度稀疏的训练数据使得模型的学习变得相当困难。

       所以存在转化公式： $cvr = ctcvr / ctr$ , 但是预估时，可能出现$ctcvr > ctr$ 导致 cvr大于1.

       ![ESMM model](https://pic3.zhimg.com/v2-0a5085c88e5e914fff367835734b1f1e_b.jpg)
       
       ![ESMM](https://pic4.zhimg.com/v2-991a0f5423618fa373edd1609fc680bf_b.jpg)

9. 画像中的建模问题？

    1. 预测结果的分布调整问题（尽可能不降低预测的acc）
    2. 预测结果的稳定性

10. 智能营销相关概念

    1. uplift model

11. 广告相关概念

     1. ctr 点击率
     2. cvr 转化率
     3. bid出价
     4. ecpm=ctcvr*bid （effective cost per mille）指的是每一千次展示可以获得的广告收入



