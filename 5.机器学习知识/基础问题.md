1. 什么是有监督训练，什么是无监督训练，举一个无监督例子

   A： 监督训练，基于的是标注数据，就是XY 特征和真值都是知道的。无监督学习，就是学习的目标不是预测$$^{\hat{Y}}$$, 而是比如聚类这种k-means，AutoEncoder这种。

2. 怎么解决过拟合问题

   ![img](https://pic3.zhimg.com/80/v2-953f6e7ce85ac1856764572e1c855705_hd.jpg) 

   - 更多的数据，比如图像识别中，经常旋转，镜像图片来增加数据量

   - L1，L2正则化

   - svm松弛变量

   - 决策树剪枝

   - dropout，随机断掉一些神经元的连接

   - Early Stopping （评价训练正确率、测试正确率曲线，提早结束训练） 

     ![img](https://pic2.zhimg.com/v2-688947d4b4f9829752c69dcaa5a7c0a0_b.jpg) 

   - 选择简单的模型（比如线性模型，浅层模型，限制gbdt的树深度）

3. L1，L2是什么，有什么区别，在什么情况下使用L1，什么情况下使用L2

   - L0 = $\sum_{j=1,\theta_{j}\neq 0}^{m}\theta_{j}^{0}$

     L0 是对参数个数进行惩罚，除非这个参数为0，否则会被惩罚

   - $L1 = \sum_{j=1}^{m}\left | \theta_{j} \right | $

     L1 (lasso regression) 是对按照参数大小的绝对值进行惩罚，菱形 一般会导致稀疏解，即很多参数会在0值取得最优

   - $L2 = \sum_{j=1}^{m} \theta_{j}^{2} $

     L2 (ridge regression) 是对按照参数大小的平方进行惩罚，圆形，会保留更多的特征

4. 什么是交叉验证

   将原始数据分成K组（一般是均分），将每个子集数据分别做一次验证集，其余的K-1组子集数据作为训练集，这样会得到K个模型，用这K个模型最终的验证集的分类准确率的平均数作为此K-CV下分类器的性能指标。K一般大于等于2，实际操作时一般从3开始取，只有在原始数据集合数据量小的时候才会尝试取2。K-CV可以有效的避免过拟合以及欠拟合状态的发生，最后得到的结果也比较具有说服性。  

5. 如何解决数据缺失的情况（丢弃、补0、补均值、补固定值、补预测值，通过我之前做的实际项目，分别阐述这样做可能会造成的后果） 

   - 删除，导致数据减少
   - 填充，（均值，平均值，众数，插值等），等于增加了误差，效果不好
   - 用其他的值利用模型预测这些值，如果其他值和这些值无关，则预测无意义，如果预测很准确，其实说明其他的值已经包含了所有特征，这些值没是意义，一般介于两者之间。
   - 映射到高维空间，加入若干维来标记是否有值，比如性别{男，女} => {是否男，是否女，是否缺失}，保留了原始信息， 维度大大增加，稀疏性大增。

6. SVM中需要解决的重要数学问题是什么（拉格朗日对偶问题 ，具体的数学内容我说不太了解，面试官说没关系），其实是应该是把有约束条件的解转化为无约束条件的解，梯度下降法使用了什么重要的数学依据（函数的局部最小最大值） 

  - 拉格朗日乘子法，需要把约束条件包含在损失函数中，转换原始带约束条件的损失函数为无约束的拉格朗日对偶问题的形式

   - 梯度下降法的依据就是求解$f^{'}(x) = 0$，得到极小值。

7. 什么是生成模型，什么是判别模型？

   判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率 

8. 为什么要做数据归一化，在梯度下降时有什么好处（加速梯度下降，减少梯度下降时的摆动，根据下降曲线进行讲解，很容易得出） 

   数据归一化，实际上是对一维连续特征进行线性变化， 不改变特征的分布，不改变模型的最优解。 有最大最小值归一化和标准化两种， 标准化更多的考虑了数据整体的分布，而最大值最小值可能会受到异常的最大值最小值影响。 数据归一化让每个特征的量纲统一，加速梯度下降的速度。但是如果量纲本来是统一的，那么没必要做归一化。

   （有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价，例如SVM ， 所以必须归一化）

9. 什么是前馈传播，什么是反向传播，推导一下反向传播算法，(手写一个三层神经网络，w11，w12,w13,w21,w22,w31。我说的时候有点把自己绕晕了，不过bp其实理解了就这么回事，实际使用时就是一行代码) 

   Answer： 待定

10. 简述下kmeans算法，如何选择k的个数（说了和业务相关），k-means++是怎么确定k的个数，面试官提示k和业务无关（++没接触过，不是很了解，他说没关系，http://www.cnblogs.com/dudumiaomiao/p/5839905.html）。

  计算每个镞的轮廓系数，这个反应了每个镞的聚合度，根据这个来定k值。

11. GBDT和随机森林的比较（BGDT是指梯度提升树）（刚好复习的时候看到过这个题，就大概说了下，一个boosting，一个是bagging，处理方式是偏差和方差等） 

12. 模型评价指标 ROC、AUC（我回答的不是很好），然后又问我精准率和召回率是什么，用0，1样本来说明（我手写推导了一下TP、TN等） 

    二分类问题在机器学习中是一个很常见的问题，经常会用到。[ROC](https://link.jianshu.com/?t=https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (Receiver Operating Characteristic) 曲线和 [AUC](https://link.jianshu.com/?t=https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) (Area Under the Curve) 值常被用来评价一个二值分类器 ([binary classifier](https://link.jianshu.com/?t=https://en.wikipedia.org/wiki/Binary_classification)) 的优劣 

    假设有N个样本，预测为两类（阳性 positive ，阴性 negative）， 其中n预测为阳性 ，m预测为阴性 ，但是n中还有一部分实际假阳性的FP，真阳性为TP， m中实际上有部分假阴性记为FN,  和真阴性TN。 FN + TN = m， TPR =  TP / (TP + FN) 预测的真阳占总真阳的概率,  FPR = FP / (FP + TN) 预测出的假阳占总真阴的概率 。每个二分类都会有个概率阈值，这个阈值越大，在召回率和正确率上有一个tradeoff， 召回率越高正确率就越低，召回率越低正确率就越高。AUC 是ROC曲线下的面积。

    ![img](http://images.cnitblog.com/blog2015/712297/201504/081953479158732.jpg)

    ![img](https://upload-images.jianshu.io/upload_images/145616-ce8221a29d9c01ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700) 

    **真正类率(TPR)** - 预测为正的正实例在所有正类中的比例： TP / (TP + FN) 

    **负正类率(FPR)** - 预测为正的负实例在所有负类中的比例 :    FP / (FP + TN)

    Note: 一般来说，threshold特别高接近1时，**TPR**和**FPR**都很低，因为P << N ; threshold特别低接近0时，**FPR**和**TPR**都很高，因为P >> N

    **召回率 (recall)** - TP / (TP + FN)  实际就是**真正类率TPR**

    **精准率 (precision)** - TP / (TP + FP), 体现了正类预测的准确性

13. 有没有用过tensorflow，（我说用过，框架不太好改，虽然有keras的封装，自己后面主要还是手写算法，或者用pytorch） 

    用过tensorflow，tensorflow比较容易的定义损失函数，spark-ml反正是不能自定义损失函数的。而且tensorflow在计算张量的时候可能合理的利用gpu和cpu，比如之前我的这个模型，gpu 12g显存不够。

14. 降维方法中 PCA 和 LDA的区别？

    首先两者都是线性的降维（相对于t-SNE），PCA是无监督的，LDA是有监督的，要考虑y值的影响。PCA仅仅考虑投影到方差最大的方向上， LDA要考虑两个类别的中心足够远，同时每类到类别中心的距离足够近。

    ![](https://pic3.zhimg.com/v2-bd7439b20305d179a263825cea0255c0_1440w.jpg?source=172ae18b)

    具体参考：

     https://zhuanlan.zhihu.com/p/137968371

     https://zhuanlan.zhihu.com/p/30037631

 其中t-SNE是让点和其他点相对距离的分布保持不变的降维方式，也是无监督的。

15. 激活函数的种类

    常用的激活函数为sigmoid、tanh、relu、leaky relu、elu。

    采用sigmoid激活函数计算量较大，而且sigmoid饱和区变换缓慢，求导趋近于0，导致梯度消失。sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。

    tanh它解决了zero-centered的输出问题，然而，gradient vanishing的问题和幂运算的问题仍然存在。

    relu从公式上可以看出，解决了gradient vanishing问题并且计算简单更容易优化，但是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新（Dead ReLU Problem）；leaky relu有relu的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明leaky relu总是好于relu。

    elu也是为解决relu存在的问题而提出，elu有relu的基本所有优点，但计算量稍大，并且没有完全证明elu总是好于relu。

16. 如何解决样本量过大，且pattern不均衡的方式？

    问题（样本选择，原型选择）是很久以前因为算力不足，需要加快训练。但目前不仅仅为了加速，可能负样本中有两种典型样本，但是其中一种相对很少，同时它在特征上和某一类正样本的很像，这导致如果仅仅是做正负样本的均衡，可能会导致这类负样本被模型误识别正样本。

    1. 模型解决： libsvm 仅仅关心边界的样本
    2. 聚类方法： 聚类后，在每个簇里面留下相同数量的不同类别的样本