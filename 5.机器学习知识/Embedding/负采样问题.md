# tf.nn.nce_loss VS tf.nn.sampled_softmax_loss?

1. 为什么要进行负采样？ 

   A： 多分类一般利用 softmax， 其公式 $ \displaystyle \sigma_i = \frac{e^{z_i}}{\displaystyle \sum_{j=1}^{n}e^{z_j}}$,  如果分类的类别数n非常大，那么计算量也会非常大。负采样主要是简化分母部分的计算。一般在训练时可以进行负采样，推理时一般需要全量计算。(老老实实做softmax计算)
   
2. tf.nn.nce_loss 是怎么做的？（对比tf.nn.sampled_softmax_loss)

   A： 进行采样，计算sampled_logits， 采用log-uniform (Zipfian) base distribution，要求按照把item_embedding 按照item frequency降序排列。

   得到sampled_logits以后，再计算 sigmoid_cross_entropy_with_logits.(可以支持num_true>1)

    tf.nn.sampled_softmax_loss 的负采样阶段都一样，仅仅是得到labels和logits以后，用softmax_cross_entropy_with_logits_v2 而不是sigmoid_cross_entropy_with_logits。sampled_softmax_loss 有pairwise的作用，而nceloss是基于加法的，所以仅仅是pointwise的效果。

  