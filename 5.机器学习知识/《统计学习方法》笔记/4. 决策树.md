# 决策树

通过对特征进行划分，将样本分成不同区域，使得每个区域的子集拥有最好的分类。下图是划分的直观做法。

![DT1](D:\工作相关\技术积累\knowledgeSummary\5.机器学习知识\《统计学习方法》笔记\DT1.png)

![DT2](D:\工作相关\技术积累\knowledgeSummary\5.机器学习知识\《统计学习方法》笔记\DT2.png)

如何体现一个划分是否让产生的子集拥有最好的分类：

1. 信息增益，多产生的信息增益越多，说明这个划分越好。**ID3**
2. 信息增益比，信息增益偏好产生子分类多的特征，所以需要用信息增益比来辅助 **C4.5**
3. GINI不纯度 **CART树 （classification and regression tree, CART)**



**信息增益**

信息熵 $$H(X) =  - \sum_{i=0}^{n} p(x) log p(x) $$

条件熵 $$H(X|Y) =  - \sum_{x,y} p(x, y) log p(x|y)  $$

【理解】 $$p(x,y)$$ 可以理解成x,y联合分布的分布密度函数， $$logp(x|y)$$ 为条件熵的熵值， $$p(x|y) $$由于引入了已知条件$$y$$，使得熵值降低，确定性增高 $$p(x|y ) < p(x)$$, 从而$$ logp(x|y) $$也相对于$$logp(x)$$降低。

【推导公式】

$$ H(X|Y) = H(X,Y) - H(Y)$$

$$H(X|Y) = - \sum_{x,y}p(x,y)logp(x,y) + \sum_{y}p(y)logp(y)$$

$$= - \sum_{x,y}p(x,y)logp(x,y) + \sum_{y}(\sum_{x}p(x,y))logp(y)$$

$$= - \sum_{x,y}p(x,y)logp(x,y) + \sum_{x, y}p(x,y)logp(y)$$ 

$$\displaystyle = - \sum_{x,y}p(x,y)log \frac{p(x,y)}{p(y)} $$ 

$$= - \sum_{x,y}p(x,y)log p(x|y) $$ 			

信息增益 = 熵 – 条件熵， 所以这里$$H(Y)$$ 是信息增益

$$G(X, Y) = H(X,Y) - H(X|Y)$$

**信息增益比** ： $$\displaystyle G_{r}(X, Y) = \frac{G(X,Y)}{H(X, Y)}$$

**GINI不纯度**

分类树 ： $$G(D) = 1 - \sum_{i=1}^{k} p_{k}^{2}$$

回归树： $$e = \sum_{i=0}^{k}(y_{i} - c)^{2}$$

------

**剪枝算法（初步理解）**： 

- 计算子树每个叶节点的熵值和 $$C(T)$$
-  $$|T|$$ 为叶节点个数，$$\alpha$$ 为控制剪枝力度的系数, 计算 $$C(T) + \alpha|T|$$
-  和剪枝后的比较，剪枝后 $$C^{'}(T) + \alpha$$ , 与剪枝前比较， 如果小则执行剪枝

