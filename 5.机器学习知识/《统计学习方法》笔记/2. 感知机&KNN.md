# 感知机

**二分类**+**线性**+**判别**  模型，学习一个超平面，把超平面两侧输出为+1和-1，是神经网络和SVM的基础.

模型目标： $$\displaystyle y = f(x) = sign(w\cdot x + b)$$

误分类点到超平面的集合距离： $$ \displaystyle \frac{ - y\cdot (w \cdot x + b)}{\left \|  w \right \|}$$ 

【理解】对于误分类来说，$$ y\cdot (w \cdot x + b)$$ 一定为负，那么$$-y\cdot (w \cdot x + b)$$为其函数距离，$$ \displaystyle\frac{ - y\cdot (w \cdot x + b)}{\left \|  w \right \|}$$ 为几何距离

损失函数： $$\displaystyle L(w, b)  = - \sum_{x \in M}^{}\frac{ y^{(i)}\cdot (w \cdot x^{(i)} + b)}{\left \|  w \right \|}$$ M为误分类点的集合

# K近邻

假设给定一个训练集，对于新的实例输入，根据其k个最近的训练实例的类别，通过投票的方式进行预测。

需要确定是，如何度量新实例与训练实例的距离。一般为$$L_{p}$$距离或者Minkowski距离。

一般认为$$p \geq 1$$, 

当p=1时, 称作曼哈顿距离 $$\displaystyle L_{1}(x_{i},x_{j}) = \sum_{l=1}^{n}|x_{i}^{(l)} - x_{j}^{(l)}|$$

当p=2时，称作欧式距离 $$\displaystyle L_{2}(x_{i},x_{j}) = \sqrt{\sum_{l=1}^{n}|x_{i}^{(l)} - x_{j}^{(l)}|^{2}}$$

当p=$$\infty ​$$, $$\displaystyle L_{\infty}(x_{i},x_{j}) = max|x_{i} - x_{j}|​$$  距离为各个维度的距离最大值

![](D:\工作文档\工作积累\knowledgeSummary\5.机器学习知识\《统计学习方法》笔记\Lp.png)





