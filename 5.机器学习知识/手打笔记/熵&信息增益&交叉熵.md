## 熵

定义： 用来度量信息的不确定程度。

解释： 熵越大，信息量越大。不确定程度越低，熵约小，比如“明天太阳从东方升起”这句话的熵为0，因为这个句话没有带有任何信息，它描述的是一个确定无疑的事情。

例子：假设有随机变量X，用来表达明天天气的情况。X可能出现三种状态 1) 晴天2) 雨天 3)阴天 每种状态的出现概率均为P(i) = 1/3，那么根据熵的公式：

![img](https://pic2.zhimg.com/80/v2-a0ab3df37332d5a6e586e925182bd79b_hd.jpg)

可以计算得到

H(X) = - 1/3 * log(1/3) - 1/3 * log(1/3) + 1/3 * log(1/3) = log3 =0.47712

如果这三种状态出现的概率为(0.1, 0.1, 0.8), 那么

H(X) = -0.1 * log(0.1) *2 - 0.8 * log(0.8) = 0.277528

可以发现前面一种分布X的不确定程度很高，每种状态都很有可能。后面一种分布，X的不确定程度较低，第三种状态有很大概率会出现。 所以对应前面一种分布，熵值很高，后面一种分布，熵值较低。

------

## 条件熵

定义：在一个条件下，随机变量的不确定性。

举例说明：

假设随机变量X表示明天的天气情况，随机变量Y表示今天的湿度，Y 有两种状态 1) 潮湿 2) 干燥。

假设基于以往的18个样本， X 的三种状态，概率均为 0.33， Y的两种状态，概率为0.5

![img](https://pic4.zhimg.com/80/v2-edb63658ac40aebb77ec6d677bbd54c7_hd.jpg)

条件概率可以通过朴素贝叶斯公式进行计算:

P(X=0|Y=0) =P(X=0,Y=0)/P(Y=0) = (1/18)/(9/18) = 1/9

P(X=1|Y=0)= P(X=1,Y=0)/P(Y=0) = (5/18)/(9/18) = 5/9

P(X=2|Y=0) =P(X=2,Y=0)/P(Y=0) = (3/18)/(9/18) = 3/9

P(X=0|Y=1) =P(X=0,Y=0)/P(Y=1) = (1/18)/(9/18) = 1/9

P(X=1|Y=1)= P(X=1,Y=0)/P(Y=1) = (5/18)/(9/18) = 5/9

P(X=2|Y=1) =P(X=2,Y=0)/P(Y=1) = (3/18)/(9/18) = 3/9

条件熵的公式：

![img](https://pic4.zhimg.com/80/v2-33483e318d4815e143d6c02a2cb97b1f_hd.jpg)

根据这个公式：

H(X|Y) = (1/18)*log(1/9) + (5/18)*log(5/9) + (3/18)*log(3/9) + (1/18)*log(1/9) + (5/18)*log(5/9) + (3/18)*log(3/9) = 0.406885

信息增益 = 熵 – 条件熵

**信息增益的定义：在一个条件下，信息不确定性减少的程度**

**所以Y条件产生的信息增益为 0.47712 - 0.406885**

信息增益的应用： 我们在利用进行分类的时候，常常选用信息增益更大的特征，信息增益大的特征对分类来说更加重要。决策树就是通过信息增益来构造的，信息增益大的特征往往被构造成底层的节点。

Q & A:

[1] 为什么H(X|Y)不是如下这个公式？

![img](https://pic2.zhimg.com/80/v2-890f64b00f59d5b784fe3c22db764786_hd.jpg)

答：首先我们需要理解熵值是怎么计算的，也就是本文的第一个公式。按照我的理解熵是一个多项加权平均值。p(xi) 就是i项对应的权重值，log(1/p(xi))就是i项对应的熵值。

那么可能有人不太明白为什么这一项的熵值 = log(1/p(xi)) ， 假设x存在四种取值，可能为[0,1,2,3]， 那么为了表达这个四种可能，需要2个bit来表示: [0b00, 0b01, 0b10, 0b11]

实际上bit数就是熵值。那么这项的熵值是2么？ 不一定，从之前的公式可以看出，熵值和这项的概率成反比，也就是p(xi) 越高， 熵值越小。因为我们可以对每种可能值进行不等长的编码，概率高的用短编码，概率低的用长编码。搞懂haffman编码就能明白这一点了。

搞明白了这个最基本公式的由来，那么回到条件熵的问题，p(x,y) 代表了每种组合的概率，无需考虑y是否已知，而计算每项熵值log p(x|y)的时候，引入**y是已知**的这个条件，可以容易看出y已知情况下，每项xi概率的增大（即每项熵值的减少）。

引入已知条件y，使得总熵值减少，这也是条件熵的意义所在。

如果上面讲的不能理解，下面还有一段从别的文章抄来的推导，我觉得也比较好理解：

- 两个随机变量X，Y的联合分布，可以形成**联合熵**（Joint Entropy），用H(X, Y)表示。

- - 即：H(X, Y) = -Σp(x, y) log(x, y)

- H(X, Y) - H(Y)

- - 表示(X, Y)发生所包含的熵，减去Y单独发生包含的熵：在Y发生的前提下，X发生新带来的熵。

- **条件熵**：H(X|Y)

![img](https://pic4.zhimg.com/80/v2-66d08f717e7c0da07e4fe5cfc8f48cfb_hd.jpg)

------

## 互信息

定义：指的是两个随机变量之间的相关程度。

理解：确定随机变量X的值后，另一个随机变量Y不确定性的削弱程度，因而互信息取值最小为0，意味着给定一个随机变量对确定一另一个随机变量没有关系，最大取值为随机变量的熵，意味着给定一个随机变量，能完全消除另一个随机变量的不确定性。这个概念和条件熵相对。

公式：

![img](https://pic2.zhimg.com/80/v2-477f57b095781c471b782f60d0e1eac7_hd.jpg)

假设X,Y完全无关，H(X) = H(X|Y) , 那么I(X;Y) = 0

假设X,Y完全相关，H(X|Y) =0， 那么I(X;Y) = H(X)

**条件熵越大，互信息越小，条件熵越小，互信息越大。**

**互信息和信息增益实际是同一个值。**

------

## 交叉熵

定义：信息论中的重要概念，主要用于度量两个概率分布间的差异性信息。

理解： 在进行优化的过程中，往往将交叉熵又命名为loss变量，优化的目标即是最小化loss。

假如X为一组已知的输入特征值，Y为一组已知的输出分类。优化的目标是为了找到一个映射模型F, 使得预测值$\hat{Y}= F(X) $ ， 与真值$Y$最相似。但现实世界的$Y$和$\hat{Y}$的分布肯定不是完全一致的。

所以：

$Y$ 服从 p 分布（即真实分布）(用经验分布近似真实分布)

$\hat{Y}$ 服从 q 分布

KL散度：相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异

KL散度公式： $$\displaystyle D_{KL}(p||q) = \sum_{i=0}^n p(x)log(\frac{p(x_i)}{q(x_i)})$$  =>  $\displaystyle \sum_{i=0}^n p(x_i)logp(x_i) - \sum_{i=0}^np (x_i)logq(x_i)$

交叉熵cross_entropy 即为描述p,q两个分布差异性的指标。

交叉熵是KL散度的变形，去掉p分布[==经验分布(根据现有数据已知)==]的左项，即得到交叉熵的公式

$$\displaystyle H(p,q) = -\sum_{i=0}^n p(y)logq(y)$$

一般令损失函数 $L(w) = H(p,q)$ ，以最小化交叉熵为损失函数

