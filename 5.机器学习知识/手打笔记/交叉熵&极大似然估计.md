**熵**

定义： 描述信息的多少

公式： $$\displaystyle H(x) = -\sum_{i=0}^np(x_i)logp(x_i)$$

n为空间中所有互斥的事件的量， i 代表 第i个事件，$ -logp(x_i)$ 表示 i 事件的信息量，H(x) 表示整个空间的信息量 （可以理解为所有事件信息量的期望）

==个人理解：霍夫曼编码就是熵的运用，用短编码来代表高频的符号，则可以使用尽可能少的bit来表达所有的可能性==

**交叉熵**

定义：信息论中的重要概念，主要用于度量两个概率分布间的差异性信息。

理解： 在进行优化的过程中，往往将交叉熵又命名为loss变量，优化的目标即是最小化loss。

假如X为一组已知的输入特征值，Y为一组已知的输出分类。优化的目标是为了找到一个映射模型F, 使得预测值$\hat{Y}= F(X) $ ， 与真值$Y$最相似。但现实世界的$Y$和$\hat{Y}$的分布肯定不是完全一致的。

所以：

$Y$ 服从 p 分布（即真实分布）(用经验分布近似真实分布)

$\hat{Y}$ 服从 q 分布

KL散度：相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异

KL散度公式： $$\displaystyle D_{KL}(p||q) = \sum_{i=0}^n p(x)log(\frac{p(x_i)}{q(x_i)})$$  =>  $\displaystyle \sum_{i=0}^n p(x_i)logp(x_i) - \sum_{i=0}^np (x_i)logq(x_i)$

交叉熵cross_entropy 即为描述p,q两个分布差异性的指标。

交叉熵是KL散度的变形，去掉p分布[==经验分布(根据现有数据已知)==]的左项，即得到交叉熵的公式

$$\displaystyle H(p,q) = -\sum_{i=0}^n p(y)logq(y)$$

一般令损失函数 $L(w) = H(p,q)$ ，以最小化交叉熵为损失函数



**分类问题下的==极大似然估计==和==最小化交叉熵==(在解决二分类问题上是等同的)**

有样本空间上的 ![[公式]](https://www.zhihu.com/equation?tex=p%28y%7Cx%29) 满足某个Categorical distribution。 有k种可能性，那么似然概率为 $\displaystyle p(y|x;p) = \prod_{k=0}^Kp_k^{y_k} $

```
p = {'rain': .14, 'snow': .37, 'sleet': .03, 'hail': .46}
那么 
p (y = 'snow' = [0,1,0,0]) = (.14)^0 * (.37)^1 * (.03)^0 * (.46)^0
```

假设有N个样本， 那么极大似然的损失函数为 $$\displaystyle L((x,y);p) = \prod_{i=0}^{n}\prod_{k=0}^{K}p_k^{y_k}$$  取log，$\displaystyle=  \sum_{i=0}^{n}(log(\prod_{k=0}^{K}p_k^{y_k})) = \sum_{i=0}^{n} \sum_{k=0}^{K}log(p_k^{y_k}) = \sum_{i=0}^{n} \sum_{k=0}^{K} y_klog(p_k)$

最小化变负号 $\displaystyle -\sum_{i=0}^{n} \sum_{k=0}^{K} y_klog(p_k)$

因为是one-hot编码，所以$y_k=0$ 的项全部是0, ==极大似然估计下的损失函数==简化为 $\displaystyle L((x,y);p) = -\sum_{i=0}^{n}(y_k=1)logp_k$

==交叉熵损失函数== $$\displaystyle H(p,q) = -\sum_{i=0}^n p(y)logq(y) = -\sum_{i=0}^n y_klog\hat{y}_k$$

$y_k$ 是训练数据(经验分布）中的概率

$\hat{y}_k$ 是预测结果中的概率

 

两者是等价的

