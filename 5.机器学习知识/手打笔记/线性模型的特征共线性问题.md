线性模型的一般公式  $$Y = \theta ^{T}X = \sum_{i=1}^{n}\theta_{i}x_i$$ (n为特征维数) 

**Q:** *为啥是 $\theta^TX$ 不是 $\theta X$ ?* 

**A:**  因为在数学上，一般**向量**都是被定义为 vertically-stacked array

$v_1 = \begin{pmatrix}1\\2\\3\end{pmatrix}$,   同样的 一个矩阵被认为是一组向量的集合  $${\begin{bmatrix} 1 &1 &1\\ 2 &2 &2 \\ 3 &3  &3 \\\end{bmatrix}} = {\begin{bmatrix} v1 & v2  &v3 \\\end{bmatrix}}$$ 

所以常见有两种写法： 

首先 $\theta$ 的形状为 $(n, 1)$ 

1） $Y = X\theta$ , 此时 $ Y $ 形状为 $(m,1)$ , 其中m为样本数， $X$ 的形状为 $(m,n)$ 也就是每一行为一个样本

2） $Y = \theta ^{T}X$ , 此时 $Y$ 的形状为$(1, m)$, 其中m为样本数，$X$ 的形状为$(n,m)$ 也就是每一列一个样本

**那么线性回归的loss ？**

$\displaystyle L = J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(h_{\theta}^{(x^i)}-y^{(i)})^2$ 

$L = (\theta^TX - Y)^T(\theta^TX - Y)$  或者 $=\frac{1}{2}(X\theta - Y)^T(X\theta - Y)$



**最小二乘解析解 怎么求？** 

$L = \frac{1}{2}(Y^T - \theta^TX^T)(Y-X\theta) = \frac{1}{2}(Y^TY -Y^TX\theta - \theta^TX^TY+\theta^TX^TX\theta)$

$\displaystyle \frac{\partial J(\theta)}{\partial \theta} = \frac{1}{2}(-X^TY-X^TY +2X^TX\theta) = 0 $

$X^TY = X^TX\theta$

$\theta = (X^TX)^{-1}X^TY$



如果$x_i, x_j$ 存在共线性，比如$x_i = kx_j$，那么$X^TX$ 非满秩，不可求逆 （这里和样本数无关，因为不管样本数是多少，都会发现$x_j$ 可以被 $x_j$表达，$X$行不满秩)

那么得到$\theta$  可能无法收敛, 模型缺乏稳定性



**L2为什么能化解特征共线带来的问题？**

$\hat{\theta} = argmin(Y-\theta^TX)^T(Y-\theta^TX) + \lambda\theta^{T}\theta$

$L = (Y-\theta^TX)^T(Y-\theta^TX) + \lambda\theta^{T}\theta = \frac{1}{2}(Y^TY -Y^TX\theta - \theta^TX^TY+\theta^TX^TX\theta + \lambda\theta^{T}I\theta) $

$\displaystyle \frac{\partial J(\theta)}{\partial \theta} = \frac{1}{2}(-X^TY-X^TY +2X^TX\theta) + (I^T + I)\theta= 0 $

=> $X^TY = X^TX\theta + I\theta$

$\hat{\theta} = (X^TX + \lambda I)^{-1}X^TY$





参考链接 ： https://cloud.tencent.com/developer/article/1553710



# $Y = X\theta$   还是   $Y = \theta ^{T}X$ ？ 