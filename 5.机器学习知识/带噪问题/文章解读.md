# 理论基础

【2017】google brain 提出 Deep neural networks easily fit random labels.

1. 无论是特征随机还是样本随机，深度模型都可以拟合。training error = 0, test error = 随机
2. 原始真实数据 + 部分random 数据， 有能力学到部分真实特征， 但泛化error上升

正则化可以降低test error，但是对 generalization error 影响不直接



# 估计错误样本

【2017】Training deep neural-networks using a noise adaptation layer

1. 用 EM 算法找 network 和 correct label


【2019】Confident Learning: Estimating Uncertainty in Dataset Labels

基本假设，noise 仅仅与 label 有关。与特征无关
$$
 \tilde{y} \ne argmax{j\in1,2,...m} P[i][j]
$$


# 迭代学习

核心思路：

1. 先用大量/全部数据 training 一个 model。
2. 根据 loss 等参数，选出大概率为 true 的 label。
3. 用 true label 重新或继续 training model。
4. 重复 2-3 步。

具体种类：

1. curriculum learning（Bengio 课程学习） [论文导读](https://zhuanlan.zhihu.com/p/114825029 "MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels") 
      - keypoint
        - 如何设定curriculum 课程
      - 优点
        - 可以加速机器学习模型的训练。在达到相同的模型性能条件下，Curriculum Learning可以加速训练，减少训练迭代步数；
        - 使模型获得更好的泛化性能，即能让模型训练到更好的局部最优值状态。

2. semi-supervised learning 

   - 基本假设

      - 无标签的类别一定归属于有标签的类别set
      - 有标签数据的标签应该都是对的
      - 无标签数据一般是类别平衡的（分布于有标签的类似）

   - 细分类别

      - self-training

         - 用有标签数据训练一个分类器，然后用这个分类器对无标签数据进行分类，这样就会产生伪标签（pseudo label）或软标签（soft label），挑选你认为分类正确的无标签样本（此处应该有一个**挑选准则**），把选出来的无标签样本用来训练分类器

      - co-training 

         - 和自我学习一样，协同学习的目的也是为了用现有的有标签的数据去标记其他数据。具体步骤如下：假设现在有数据集，每组数据有2个特征(feature): x1 和 x2。我们把其中有标签的那部分数据集称为L （表示为[x1,x2,y]），没有标签的数据集称为U (表示为[x1,x2])。

           1. 初始化数据，把L 分为L1 ([x1,y]) 和L2 ([x2,y])。这里注意，L 是一个数据集，里面包涵了很多数据，我就顺便提醒一下。

           2. 重复：

              + 分别用L1和L2训练出一个模型 F1和F2

              + 分别用模型F1和F2去预测U (给U打标签)，同样这里也是只选出最有把握的一些结果。

              + 把F1预测的结果放入L2，把F2预测的结果放入L1 (交叉放置)

              + 更新L 和 U

           3. 直到数据集不发生变化 (和自我学习一样)




